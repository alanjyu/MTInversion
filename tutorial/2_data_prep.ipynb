{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation and Synthetics for Moment Tensor Inversion\n",
    "\n",
    "We have a few more steps to go through before inversion.\n",
    "\n",
    "One important step is to create synthetic Green's functions (aka synthetic seismograms) from wavenumber integration (FK). This type of method generates complete synthetic seismograms, we will use the software package ***Computer Programs in Seismology*** by R.B Herrmann ([Link to software](http://www.eas.slu.edu/eqc/eqccps.html)).\n",
    "\n",
    "### Objectives\n",
    "\n",
    "- Filter and cut the seismograms\n",
    "- Calculate synthetic Green's functions\n",
    "- Filter the synthetic Green's functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from obspy.core import read, UTCDateTime, Stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data\n",
    "The final step of data preparation is to filter and cut the seismograms. After reading the processed SAC files into ObsPy we will:\n",
    "\n",
    "* Filter and taper data\n",
    "* Down-sample data to desired sampling interval\n",
    "* Cut data relative to a reference time (e.g. origin, first arrival, etc.)\n",
    "* Convert from meters to centimeters (tdmtpy units are in dyne-cm)\n",
    "* Save the final waveforms in SAC (required)\n",
    "\n",
    "\n",
    "Let's start by reading in the processed waveforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "evid = '40191336' # event of interest\n",
    "event_dir = evid\n",
    "infile = '%s/datetime.csv'%event_dir # we need the event origin time\n",
    "station_file = '%s/station.csv'%event_dir\n",
    "\n",
    "sacdir = '%s/sac'%event_dir # location of processed data\n",
    "outdir = '%s'%event_dir # location of filtered/cut/down-sampled data for inversion\n",
    "    \n",
    "# Check if data directory exist\n",
    "P = Path(sacdir)\n",
    "if P.exists():\n",
    "    # Read event info and station info into Pandas table\n",
    "    df = pd.read_csv(infile,parse_dates=True)\n",
    "    station_df = pd.read_csv('%s'%(station_file),parse_dates=True,dtype={'location':str},na_filter=False)\n",
    "    \n",
    "    origin_time = UTCDateTime(df['origin'][0])\n",
    "    st = Stream()\n",
    "    for _,row in station_df.iterrows():\n",
    "        st += read('%s/%s.%s.%s.%s[%s]'%(\n",
    "            sacdir,row.network,row.station,row.location,row.channel,row.component),format='SAC')\n",
    "else:\n",
    "    print('Directory %s does not exist. %s does not have instrument corrected data.'%(sacdir,evid))\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter\n",
    "\n",
    "The next cell shows the processing parameters you need to define, you may need to change them for different events. Synthetic Green's functions must have the same filter, reduction velocity and sampling interval as the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter\n",
    "freqmin = 0.02\n",
    "freqmax = 0.05\n",
    "corners = 3\n",
    "\n",
    "# Desired sampling interval\n",
    "dt = 1.0\n",
    "\n",
    "# Reduction velocity in km/sec, 0 sets the reference time to origin time\n",
    "vred = 0\n",
    "\n",
    "# time before and after reference time, data will be cut before and after the reference time\n",
    "time_before = 30\n",
    "time_after = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if vred:\n",
    "    p = 1/vred\n",
    "else:\n",
    "    p = 0\n",
    "    \n",
    "st.filter('bandpass',\n",
    "          freqmin=freqmin,freqmax=freqmax,\n",
    "          corners=corners,zerophase=True)\n",
    "st.taper(max_percentage=0.05)\n",
    "\n",
    "# Trim and decimate the data\n",
    "for tr in st:\n",
    "    tr.decimate(factor=int(tr.stats.sampling_rate*dt),\n",
    "                strict_length=False,\n",
    "                no_filter=True)\n",
    "    tr.resample(1/dt, strict_length=False, no_filter=True)\n",
    "    tr.stats.sac.t1 = origin_time + p*(tr.stats.sac.dist) # set reference time\n",
    "    tr.trim(tr.stats.sac.t1-time_before,tr.stats.sac.t1+time_after,pad=True,fill_value=0)\n",
    "    tr.data = 100*tr.data # m/s to cm/s\n",
    "    tr.stats.sac.b = -1*(origin_time - tr.stats.starttime)\n",
    "    tr.stats.sac.o = 0\n",
    "    # Save final trace using tdmtpy file name format\n",
    "    sacout = '%s/%s.%s.dat'%(outdir,tr.id[:-4],tr.id[-1])\n",
    "    tr.write(sacout,format='SAC')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Green's Functions\n",
    "\n",
    "Now is time to calculate the synthetic Green's functions\n",
    "* Execute the FK calculation\n",
    "* Apply the same filter to the synthetics\n",
    "* Save them to the appropriate format for inversion\n",
    "\n",
    "The FK calculation requires two input files, a velocity model file and a distance file. A velocity model file `gil7.d` is provided, this is a 1-D model for northern California. We will create the distance file `dfile` from the Pandas table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = 'gil7'\n",
    "#depths = round(df['depth'][0]) # Only compute GFs at catalog depth\n",
    "depths = sorted([10,20,round(df['depth'][0])]) # compute GF at 10, 20 km and at catalog depth\n",
    "npts = int(256) # number of points in the time series, must be a power of 2\n",
    "t0 = int(0) # used to define the first sample point, t0 + distance_in_km/vred\n",
    "\n",
    "# location of synthetic Green's functions\n",
    "green_dir = '%s/%s'%(event_dir,model)\n",
    "Path(green_dir).mkdir(parents=True,exist_ok=True)\n",
    "    \n",
    "for depth in depths:\n",
    "    # Create distance file\n",
    "    dfile = ('{dist:.0f} {dt:.2f} {npts:d} {t0:d} {vred:.1f}\\n')\n",
    "    dfile_out = '%s/dfile'%event_dir\n",
    "    with open(dfile_out,'w') as f:\n",
    "        for _,row in station_df.iterrows():\n",
    "            f.write(dfile.format(dist=row.distance,dt=dt,npts=npts,t0=t0,vred=vred))\n",
    "\n",
    "    # Generate the synthetics\n",
    "    os.system('hprep96 -M %s.d -d %s -HS %.4f -HR 0 -EQEX'%(model,dfile_out,depth))\n",
    "    os.system('hspec96')\n",
    "    os.system('hpulse96 -D -i > file96')\n",
    "    os.system('f96tosac -B file96')\n",
    "\n",
    "    # Filter and save the synthetic Green's functions\n",
    "    greens = ('ZDD','RDD','ZDS','RDS','TDS','ZSS','RSS','TSS','ZEX','REX')\n",
    "\n",
    "    for index,row in station_df.iterrows():      \n",
    "        for j,grn in enumerate(greens):\n",
    "            sacin = '%s/%s.%s.dat'%(outdir,tr.id[:-4],tr.id[-1])\n",
    "            sacout = '%s/%s.%s.%s.%.4f'%(green_dir,row.network,row.station,row.location,depth)\n",
    "            tmp = read(sacin,format='SAC')\n",
    "            tmp.filter('bandpass',freqmin=freqmin,freqmax=freqmax,corners=corners,zerophase=True)\n",
    "            tmp.write('%s.%s'%(sacout,grn),format='SAC') # overwrite\n",
    "\n",
    "# Uncomment to remove unfiltered synthetic SAC files\n",
    "os.system('rm B*.sac') # remove the unfiltered SAC files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create input file for MTtime\n",
    "\n",
    "Now that we have prepared the data and synthetics for inversion, we can create the input file for tdmtpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   station  distance  azimuth  ts  npts   dt  used  longitude  latitude\n",
      "BK.QRDG.00     80.99   335.29  30   150 1.00     1    -122.14     38.48\n",
      "BK.RUSS.00     81.16   353.18  30   150 1.00     1    -121.87     38.54\n",
      " BK.CVS.00     84.88   313.73  30   150 1.00     1    -122.46     38.35\n",
      "BK.OAKV.00     88.89   320.02  30   150 1.00     1    -122.41     38.43\n",
      "BK.MCCM.00    105.12   290.48  30   150 1.00     1    -122.88     38.14\n",
      "BK.FARB.00    110.46   263.41  30   150 1.00     1    -123.00     37.70\n",
      "BK.WELL.00    113.71    52.46  30   150 1.00     1    -120.72     38.44\n",
      " BK.SAO.00    120.23   166.71  30   150 1.00     1    -121.45     36.76\n",
      " BK.CMB.00    122.83    78.33  30   150 1.00     1    -120.39     38.03\n",
      "BK.MNRC.00    132.06   333.21  30   150 1.00     1    -122.44     38.88\n",
      " BK.SCZ.00    139.07   166.84  30   150 1.00     1    -121.40     36.60\n",
      "BK.BUCR.00    142.56    96.01  30   150 1.00     1    -120.15     37.67\n"
     ]
    }
   ],
   "source": [
    "# Create headers\n",
    "headers = dict(datetime=df['origin'][0],\n",
    "               longitude=df['lon'][0],\n",
    "               latitude=df['lat'][0],\n",
    "               depth=','.join([ '%.4f'%d for d in depths]),\n",
    "               path_to_data=event_dir,\n",
    "               path_to_green=green_dir,\n",
    "               green='herrmann',\n",
    "               components='ZRT',\n",
    "               degree=5,\n",
    "               weight='distance',\n",
    "               plot=0,\n",
    "               correlate=0,\n",
    "              )\n",
    "\n",
    "# Add station table\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "frame = {'station': station_df[['network','station','location']].apply(lambda x: '.'.join(x),axis=1)}\n",
    "df_out = pd.DataFrame(frame)\n",
    "df_out[['distance','azimuth']] = station_df[['distance','azimuth']]\n",
    "df_out['ts'] = int(30)\n",
    "df_out['npts'] = int(150)\n",
    "df_out['dt'] = dt\n",
    "df_out['used'] = 1\n",
    "df_out[['longitude','latitude']] = station_df[['longitude','latitude']]\n",
    "print(df_out.to_string(index=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save to file `mtinv.in` for the inversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'cat' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# write\n",
    "with open('mtinv.in','w') as f:\n",
    "    for key, value in headers.items():\n",
    "        f.write('{0:<15}{1}\\n'.format(key,value))\n",
    "    f.write(df_out.to_string(index=False))\n",
    "    \n",
    "!cat mtinv.in"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start the next tutorial and take a look at the moment tensor inversion package `mttime`."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
